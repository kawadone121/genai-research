# 製造業向け生成AI開発：暗黙知の統合に関する調査

本プロジェクトは、工場用機械に関する高度な問い合わせに対応できる生成AIの開発を目指しています。専門技術者が持つ「暗黙知（経験則、勘、文脈理解）」をAIに学習・適用させるための手法を体系化しました。

## 背景と目的
- **目的**: 顧客からの機械トラブルや仕様に関する問い合わせに対し、熟練者レベルの回答を自動生成する。
- **課題**: マニュアルにある「形式知」だけでは解決できない事例が多く、ベテラン技術者の頭の中にある「暗黙知」の活用が必要不可欠である。

---

## 暗黙知を教え込むための主要手法カテゴリ

暗黙知をAIに統合するためのアプローチを4つのカテゴリに分類して整理します。

### 1. ナレッジエンジニアリング・RAG (暗黙知の形式知化と検索)
ベテランの脳内にある知識を言語化・構造化し、AIが参照可能なデータとして外部化するアプローチです。

- **概要・イメージ**:
  インタビューや過去の対応履歴から「このパターンのときはここを見る」といった判断基準を洗い出し、ドキュメントやナレッジグラフ化する。AIはその外部外部知識を検索（RAG）して回答を作成する。
  > *イメージ: 熟練者の横に優秀な司書(AI)がいて、熟練者が書いた秘伝のメモを都度渡してくれる状態。*

- **具体的な手法名**:
  - **Advanced RAG**: チャンク最適化、ハイブリッド検索（キーワード＋ベクトル）。
  - **GraphRAG / Knowledge Graph**: 部品、症状、原因などの関係性をグラフ構造化し、「Aの症状ならBも疑う」といった関連性を辿れるようにする。
  - **インタビューベースのナレッジ抽出**: 専門家へのヒアリング内容をQAリスト化してDBに登録。

- **扱える暗黙知**:
  - 「マニュアルにはないが、この部品は気温変化に弱い」といった特定の因果関係。
  - 複雑なトラブルシュートの手順・分岐。
  - 製品間の互換性に関する現場レベルの知識。

- **メリット**:
  - **ハルシネーション抑制**: 根拠データに基づいた回答が可能。
  - **更新性**: 知識が変わってもデータを差し替えるだけで済む（再学習不要）。
  - **透明性**: どの情報を元に回答したか出典を示せる。

- **デメリット**:
  - **データ化のコスト**: 暗黙知を言語化・構造化する初期工数が非常に大きい。
  - **検索精度の限界**: 質問のニュアンスがズレると適切な情報がヒットしない場合がある。

---

### 2. Fine-tuning (モデルへのパターン注入)
AIモデル自体のパラメータを更新し、専門家のような言い回しや思考の癖を直接覚え込ませるアプローチです。

- **概要・イメージ**:
  大量の過去の良質な回答データセットを学習させ、モデルの「脳」自体を製造業特化型に作り変える。
  > *イメージ: 新入社員(AI)を現場に配属し、過去数年分の全対応履歴を読破させて、先輩の口調や勘を肌感覚として覚えさせる状態。*

- **具体的な手法名**:
  - **SFT (Supervised Fine-Tuning)**: 質問と理想的な回答のペアで追加学習。
  - **PEFT / LoRA**: 効率的にパラメータの一部だけを調整する軽量な学習手法。
  - **Domain Adaptive Pre-training**: 製造業の技術文書や社内文書で事前学習を追加で行う。

- **扱える暗黙知**:
  - 業界特有の専門用語の正しい用法やニュアンス。
  - 専門家特有の「回答のトーン＆マナー」（丁寧さ、確信度の伝え方）。
  - 論理的説明が難しい直感的なパターン認識（「この異音なら大体あれが原因」）。

- **メリット**:
  - **応答速度**: 外部検索なしで即答できるため高速。
  - **振る舞いの模倣**: 専門家らしい自然な対話スタイルを獲得しやすい。
  - **コンテキスト理解**: 検索キーワードになりにくい文脈の空気を読む力が向上する。

- **デメリット**:
  - **コストと時間**: 高品質な学習データの作成（クリーニング）と計算リソースが必要。
  - **知識の固定化**: 新製品が出た場合など、情報の更新には再学習が必要。
  - **ハルシネーション**: 事実ではないことも自信満々に答えてしまうリスクがある。

---

### 3. Prompt Engineering (推論プロセスの明示化)
モデル自体はいじらず、入力（質問）に対する「指示」を工夫することで、専門家の思考プロセスを再現させるアプローチです。

- **概要・イメージ**:
  「熟練者ならどう考えるか」という思考の順序や観点をプロンプトとしてAIに与え、その通りに推論させる。
  > *イメージ: 優秀だが経験の浅い社員(AI)に対し、横からベテランが「まずは型番を確認して、次に異音の種類を聞いて…」と耳元で指示出しする状態。*

- **具体的な手法名**:
  - **Few-Shot Prompting**: 専門家が良い回答をした例（思考過程含む）をいくつか提示してから回答させる。
  - **Chain-of-Thought (CoT)**: 「ステップバイステップで考えて」と指示し、論理的飛躍を防ぐ。
  - **Role Prompting**: 「あなたは20年の経験を持つ工場の保全担当者です」と役割を与える。

- **扱える暗黙知**:
  - トラブルシューティングの定石となる思考フロー（まず安全確認、次に電源確認など）。
  - 顧客へのヒアリング項目（回答に必要な不足情報の特定）。
  - 専門家としての振る舞いや心構え。

- **メリット**:
  - **手軽さ**: 学習や開発コストが低く、試行錯誤が容易。
  - **柔軟性**: 指示を変えるだけで挙動を調整できる。

- **デメリット**:
  - **コンテキストの限界**: 入力できる文字数に制限があり、大量の知識は渡せない。
  - **安定性**: プロンプトの微妙な違いで回答品質がブレやすい。

---

### 4. Human Alignment (フィードバックによる価値観の学習)
生成された回答に対して人間が評価を行い、そのフィードバックループを回すことで、「何が良い回答か」という価値観を教え込むアプローチです。

- **概要・イメージ**:
  AIの回答に対し、熟練者が「それは危ない」「もっと詳しく」と採点・修正し、その結果を報酬として強化学習させる。
  > *イメージ: 社員(AI)の実践練習に対し、師匠が一つ一つ「今の回答は80点」「それは0点」とダメ出しをして矯正していく状態。*

- **具体的な手法名**:
  - **RLHF (Reinforcement Learning from Human Feedback)**: 人間のフィードバックによる強化学習。
  - **DPO (Direct Preference Optimization)**: 好みの回答ペアデータを使ってより直接的にモデルを最適化する手法。

- **扱える暗黙知**:
  - 「正解はないが、こちらの言い回しの方が顧客に安心感を与える」といった選好。
  - 企業の倫理規定や安全基準（安全サイドに倒す判断など）。
  - 微妙なニュアンスの良し悪し。

- **メリット**:
  - **アライメント**: 人間（企業の意図）の好みに合った回答を出せるようになる。
  - **安全性**: 不適切な回答や危険なアドバイスを抑制できる。

- **デメリット**:
  - **高コスト**: 専門家による大量の評価・フィードバックが必要で、運用負荷が高い。
  - **主観のバイアス**: 評価者の個人的な好みに偏る可能性がある。

---

## 推奨される統合戦略
実際のシステム開発では、これらを単独で使うのではなく、組み合わせるのが一般的です。

1. **RAG**で最新製品情報や過去事例の事実関係（形式知）を担保する。
2. **Prompt Engineering**で診断フローの手順（プロセス知）を強制する。
3. 必要に応じて**Fine-tuning**で、業界用語や専門家らしいトーン（暗黙的なスタイル）を調整する。